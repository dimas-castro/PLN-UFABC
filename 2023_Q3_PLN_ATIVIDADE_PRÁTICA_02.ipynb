{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D7hJlilKM485"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimas-castro/PLN-UFABC/blob/main/2023_Q3_PLN_ATIVIDADE_PR%C3%81TICA_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 02 [Extração e Pré-processamento de Dados + Expressões Regulares]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 02** deve ser feita utilizando o **Google Colab** com uma conta\n",
        "sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/83JggUJ1mhgWviEaA\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia 20/10 (sexta-feira) APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "Dimas de Castro Filho\n",
        "\n",
        "RA: 11201811282"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIVRO**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português.`\n",
        "\n",
        ">\n",
        "\n",
        "Disponível gratuitamente em:\n",
        "  \n",
        "  > https://brasileiraspln.com/livro-pln/1a-edicao/.\n",
        "\n",
        "\n",
        "**POR FAVOR, PREENCHER OS CAPITULOS SELECIONADOS PARA A SUA EQUIPE:**\n",
        "\n",
        "`Primeiro capítulo: 10 - Semântica Distribucional`\n",
        "\n",
        "`Segundo capítulo: 16 - Recuperação de Informação`\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` para identificar ERROS em 2 (DOIS) capítulos do livro **Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português**.\n",
        "\n",
        "Os capítulos devem ser selecionados na seguinte planilha:\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1ZutzQ3v1OJgsgzCvCwxXlRIQ3ChXNlHNvB63JQvYsbo/edit?usp=sharing\n",
        "\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC.\n",
        "\n",
        ">\n",
        "\n",
        "\n",
        "**DICA:** Por favor, insira o seu nome ou da sua equipe na ordem definida na planilha. Por exemplo, se a linha correspondente ao o GRUPO 5 já foi preenchida, a próxima equipe (GRUPO 6) deverá ser informada na próxima linha da planilha.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TIPOS DE ERROS**\n",
        "---\n"
      ],
      "metadata": {
        "id": "eD_AJQhrwJQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE**: consulta feita no ChatGPT\n",
        ">\n",
        "\n",
        "Um `programa Python` que utilize `expressões regulares` pode ajudar a identificar vários **tipos de erros** comuns em **livros**, especialmente erros de formatação e problemas relacionados à consistência do texto. Aqui estão alguns exemplos de erros comuns que podem ser identificados usando expressões regulares:\n",
        "\n",
        "* Erros de gramática e ortografia: erros de digitação, concordância verbal e nominal, uso incorreto de pontuação e outros erros gramaticais.\n",
        "\n",
        "* Problemas de formatação: você pode usar expressões regulares para encontrar erros de formatação, como espaços em excesso, tabulações inadequadas ou alinhamentos inconsistentes.\n",
        "\n",
        "* Abreviações e acrônimos: você pode usar expressões regulares para encontrar abreviações ou acrônimos que não foram definidos ou explicados anteriormente no texto.\n",
        "\n",
        "* Citações e referências: expressões regulares podem ser úteis para localizar citações ou referências que precisam de formatação especial.\n",
        "\n",
        "* OUTROS TIPOS DE ERROS: não considerem apenas os tipos de erros citados acima.\n",
        "\n",
        "\n",
        "**IMPORTANTE:** Lembre-se de que expressões regulares podem ser poderosas, mas também complexas. Dependendo da complexidade dos erros que você deseja identificar, pode ser necessário ajustar as expressões regulares de acordo com as características específicas do seu texto. Além disso, é importante ter em mente que as expressões regulares podem não ser a melhor ferramenta para todos os tipos de erros em livros, especialmente problemas mais contextuais ou semânticos, que podem exigir abordagens de PLN mais avançadas.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gz0DTI0KYmn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A equipe que **realizar mais testes** e/ou **identificar mais erros** terá o peso diminuido na AVALIAÇÃO (Prova Escrita) em **25%** (caindo de 40 para 30). Os testes e possíveis erros devem ser contabizados de maneira separada.\n",
        "\n",
        ">\n",
        "\n",
        "Além disso, **por se tratar de um livro**, há um teste importante que deve ser feito. Lembre-se que o teste deve ser feito utilizando expressões regulares. A equipe que realizar esse teste, mesmo que o erro não ocorra nos capítulos selecionados, terá o peso diminuido na AVALIAÇÃO (Prova Escrita) em **25%** (caindo de 40 para 30).\n",
        "\n",
        "> A equipe pode considerar outros capítulos do livro para tentar identificar esse tipo de erro.\n",
        "\n",
        "**Se for a mesma equipe, o peso da avaliação será reduzido em 50% (caindo de 40 para 20)**.\n",
        "\n",
        ">\n",
        "\n",
        "**IMPORTANTE**: a diminuição no peso da AVALIAÇÃO será aplicado para todos os membros da equipe. Esse critério será aplicado apenas para uma equipe, considerando como critério de desempate a equipe que entregar primeiro a atividade no formulário.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inclusão das biblotecas utilizadas para a execução do algoritmo\n",
        "#As bibliotecas requests e BeautifulSoup foram utilizadas para a obtenção do\n",
        "#conteúdo a ser analisado, nesse caso os capítulos 10 e 16, como detalhado acima.\n",
        "#As biblioteca random foi utlizada para alternar entre os user agents nas conexões.\n",
        "#A biblioteca re foi utilizada para procurar pelos termos que continham as\n",
        "#expressões regulares no corpus obtido.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import re"
      ],
      "metadata": {
        "id": "RyUailD5vi9E"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#URL dos capitulos do livro\n",
        "capitulo10_url = 'https://brasileiraspln.com/livro-pln/1a-edicao/parte5/cap10/cap10.html'\n",
        "capitulo16_url = 'https://brasileiraspln.com/livro-pln/1a-edicao/parte8/cap16/cap16.html'\n",
        "\n",
        "#Lista de user agents utilizada para fazer as conexões\n",
        "user_agents_list = [\n",
        "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
        "]"
      ],
      "metadata": {
        "id": "-ZWSvgc7fDV5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(capitulo10_url, headers={'User-Agent': random.choice(user_agents_list)})\n",
        "soup = BeautifulSoup(response.content, 'html.parser', from_encoding='ut8')\n",
        "\n",
        "if response.status_code == 200:\n",
        "  print('Conexão bem-sucedida')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhiZOhiMeKG0",
        "outputId": "2ab333d1-10e2-4834-db85-1a10820744a8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conexão bem-sucedida\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_16 = requests.get(capitulo10_url, headers={'User-Agent': random.choice(user_agents_list)})\n",
        "soup_16 = BeautifulSoup(response_16.content, 'html.parser', from_encoding='ut8')\n",
        "\n",
        "if response_16.status_code == 200:\n",
        "  print('Conexão bem-sucedida')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKQYQIe3f9Z_",
        "outputId": "ff90fb8e-20c2-40eb-aeb5-fd428e02a13f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conexão bem-sucedida\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tag necessária para extração do texto evitando imagens e fórmulas contidas.\n",
        "def sem_span(tag):\n",
        "    return tag.name == 'p' and not tag.find('span', class_='math display') and not tag.find('span', class_='math inline')"
      ],
      "metadata": {
        "id": "7fn5PQR4tp8m"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extração do conteudo do capitulo 10\n",
        "main = soup.find('main', class_='content')\n",
        "paragrafos = []\n",
        "for paragrafo in main.find_all(sem_span):\n",
        "  paragrafos.append(paragrafo.text)\n",
        "\n",
        "for i in paragrafos:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmjJPhXngf5P",
        "outputId": "1ee19c92-6dbf-4c4f-96b9-20cffad9fc4c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eloize Seno \n",
            "Daniela Claro \n",
            "Laila Mota \n",
            "Jessica Rodrigues \n",
            "26/09/2023\n",
            "É relativamente fácil para nós, seres humanos, visualizarmos um texto e, a partir de uma simples leitura, extrairmos dele determinados tipos de informação. Por exemplo, ao ler o texto “Ser feliz sem motivo é a forma mais autêntica de felicidade.” podemos reconhecer o sentido das palavras e o significado do texto formado pela junção de todas as palavras. Diferente dos humanos, os algoritmos computacionais não conseguem processar símbolos/palavras. Ao invés disso, eles requerem uma representação numérica de um documento ou texto a ser processado, para que consigam realizar suas operações.\n",
            "A semântica distribucional tem sido atualmente a principal abordagem de representação do significado lexical adotada nas mais diversas tarefas do processamento de linguagem natural. Nessa abordagem, os itens lexicais (palavras) são representados por meio de vetores de valores reais, conhecidos por vetores semânticos, que codificam o significado das palavras a partir de sua distribuição em textos.\n",
            "A semântica distribucional é ancorada na Hipótese Distribucional (Firth, 1957; Harris, 1954) que preconiza que palavras que têm um contexto linguístico semelhante tendem a ter significado similar ou aproximado. É o caso, por exemplo, de palavras como “ensino” e “educação” que costumam aparecer no mesmo contexto de palavras como “aluno”, “escola” e “professor”, sugerindo que existe uma similaridade entre as duas palavras em certos contextos. Vejamos, por exemplo, a ocorrência dessas palavras nas sentenças a seguir1:\n",
            "Assim, na semântica distribucional as palavras são caracterizadas pelo contexto em que elas aparecem. Por se basearem em distribuição, os vetores semânticos podem ser aprendidos automaticamente a partir de textos, sem que haja supervisão de um humano (utilizando-se textos não rotulados, portanto). Os modelos que aprendem esse tipo de representação são denominados de Modelos Semânticos Distribucionais - MSD (Distributional Semantic Models – DSM, no inglês).\n",
            "Os MSD são, frequentemente, classificados como vetores esparsos e vetores densos. Por exemplo, o modelo TF-IDF, amplamente adotado em tarefas que envolvem a comparação de similaridade entre documentos, como a detecção de plágio, a inferência textual e a recuperação de informações, é um exemplo clássico de vetor esparso. Nesse modelo, o significado de uma palavra é representado por meio de uma função simples calculada com base na frequência da palavra em uma coleção de documentos, conforme será visto Seção 10.2.1. Como muitas palavras nunca ocorrem em alguns documentos, frequentemente, esse modelo leva a vetores muito grandes e esparsos, ou seja, com muitos zeros. Por outro lado, os modelos da família Word2Vec (Mikolov et al., 2013) são considerados vetores densos (não esparsos), onde as entradas são números reais que representam propriedades semânticas úteis (conforme será abordado na Seção 10.3.1, ao invés de contagens quase zero.\n",
            "A Figura 10.1 ilustra a estrutura geral dos Modelos Semânticos Distribucionais. Neste capítulo serão abordados apenas os vetores esparsos e os vetores densos estáticos. Mais especificamente, iniciaremos apresentando os modelos esparsos TF-IDF e PMI (Seção 10.2) e depois apresentaremos os principais vetores densos estáticos como o Word2Vec, o GloVe e o Fasttext (Seção 10.3). Os vetores densos dinâmicos, por sua vez, como ELMO, BERT e GPT serão abordados no Capítulo 15. Antes, porém, de apresentar os MSD, introduziremos alguns conceitos fundamentais da semântica vetorial e apresentaremos a similaridade do cosseno (Seção 10.1), uma maneira padrão de usar os vetores semânticos para calcular a similaridade entre palavras, sentenças e documentos, que é uma ferramenta fundamental em aplicações práticas como a sumarização automática e a recuperação de informações.\n",
            "\n",
            "As primeiras investigações no campo da semântica vetorial, também conhecida por métodos distribucionais, tiveram início na década de 1950, impulsionadas pela convicção de que o significado de uma palavra pode ser definido a partir da sua distribuição nos contextos linguísticos em que ela ocorre, compartilhada por linguistas como (Joos, 1950), (Harris, 1954) e (Firth, 1957), e pela proposta de (Osgood; Suci; Tenenbaum, 1957) de usar um ponto no espaço multidimensional para representar a conotação de uma palavra. Portanto, os métodos distribucionais se definem como uma representação vetorial que retrata o significado de uma palavra a partir da distribuição das palavras que formam o seu contexto (Jurafsky; Martin, 2023). Por exemplo, a Figura 10.2 representa o espaço vetorial semântico da palavra “ensino”2. Palavras como “educação”, “estudantes”, “professores” e “alunos” são alguns exemplos de palavras que compartilham esse mesmo espaço semântico.\n",
            "\n",
            "A representação vetorial semântica, ou simplesmente vetores semânticos, é um padrão de representação muito usual em PLN, que pode retratar vários aspectos do significado das palavras, como a similaridade (ex. “comércio” e “negócio”); a orientação de sentimento ou polaridade (ex. “fenomenal”, que conota uma avaliação positiva, e “estúpido”, que conota uma avaliação negativa); a associação entre palavras (ex. “futebol” e “bola”, que são claramente relacionados, uma vez que futebol se joga com uma bola), entre outros aspectos.\n",
            "A ideia dos vetores semânticos é, então, representar cada palavra como um ponto em um espaço vetorial multidimensional, construído a partir da distribuição de suas palavras vizinhas.\n",
            "Espaços vetoriais são objetos de estudo da Álgebra Linear e são bem caracterizados pela sua dimensão, que, grosseiramente falando, representa o número de direções independentes no espaço. Um espaço vetorial é formado por uma coleção de objetos chamados vetores. Em um Modelo Semântico Distribucional é possível representar palavras, sentenças e até documentos completos como vetores em um espaço multidimensional.\n",
            "Geralmente, os vetores semânticos são representados por meio de uma matriz de coocorrência (ou distribuição de coocorrência), que retrata a frequência de coocorrência das palavras. As representações matriciais mais comuns são a matriz termo-documento, onde cada dimensão (vetor) da matriz representa um documento, e a matriz termo-contexto, onde cada dimensão representa uma palavra (Jurafsky; Martin, 2023). As subseções a seguir abordam essas duas formas básicas de representação.\n",
            "A matriz termo-documento foi definida por (Salton; Allan, 1994) como parte do Modelo de Espaço Vetorial – MEV proposto pelos autores para a recuperação de documentos na web. No MEV, um documento é representado como um vetor de frequência de palavras (ou termos), uma coluna, como no exemplo da Tabela 1. Os três documentos representados na tabela, por meio de colunas, representam pontos em um espaço tridimensional. A matriz foi computada a partir de três textos de divulgação científica extraídos da revista online Pesquisa FAPESP4, da seção de Tecnologia, edições 308 e 310 de 2021. Os textos se referem aos seguintes temas: energias renováveis5 (coluna 1), risco de escassez de energia fornecida pelas hidrelétricas brasileiras6 (coluna 2) e veículos elétricos movidos a etanol7 (coluna 3). A matriz da Tabela 10.1 representa apenas um subconjunto das palavras que ocorreram nesses textos. A contagem (frequência) das palavras em cada texto foi realizada considerando a sua forma lematizada.\n",
            "\n",
            "\n",
            "Em aplicações reais, os vocabulários têm milhares de palavras e o número de documentos pode ser enorme (imagine todas as páginas da web). Isso frequentemente resulta em vetores muito grandes, levando a matrizes esparsas, já que muitas palavras nunca aparecem em outros documentos. Para lidar com um grande número de dimensões, uma técnica comumente usada é a Análise Semântica Latente (em inglês, Latent Semantic Analysis – LSA), que reduz a dimensionalidade do espaço vetorial através da Decomposição em Valores Singulares (em inglês, Singular Value Decomposition – SVD), conforme veremos na Seção 10.2.3.\n",
            "Os vetores semânticos também podem ser usados para representar o significado de palavras e não apenas de documentos. Para tanto, ao invés de usarmos uma matriz de termo-documento (como visto na Seção 10.1.1), usamos uma matriz de termo-contexto, também conhecida por matriz palavra-palavra ou matriz termo-termo.\n",
            "A Tabela 10.2 apresenta um subconjunto simplificado da matriz de coocorrência termo-contexto computada a partir dos textos da revista Pesquisa FAPESP descritos na Seção 10.1.1. As linhas da tabela representam cada palavra-alvo e cada célula indica o número de vezes que a palavra-alvo correspondente coocorreu em cada contexto (colunas). Para a criação da matriz foi considerada uma janela de contexto de tamanho 5, isto é, cinco palavras à esquerda e cinco palavras à direita da palavra-alvo. Para exemplificação, o vetor da palavra-alvo “energia” está destacado em negrito.\n",
            "\n",
            "\n",
            "Os vetores que representam palavras são frequentemente denominados de embeddings, embora muitas vezes esse termo seja usado de maneira mais restrita para se referir apenas aos vetores densos como é o caso do Word2Vec, Glove e Fastext (Seção 10.3), e não aos vetores esparsos como o TF-IDF e o PMI (Seção 10.2).\n",
            "Uma tarefa bastante comum do processamento de linguagem natural consiste em calcular a similaridade entre vetores de documentos ou vetores de palavras, seja para estabelecer uma métrica de semelhança entre dois textos ou para se ter uma medida de equivalência entre duas palavras. Para tanto, faz-se necessário o emprego de alguma medida de similaridade entre vetores.\n",
            "A medida do Cosseno, também conhecida por distância do Cosseno, é sem dúvida uma das mais clássicas da área de PLN. Essa medida calcula a distância entre dois vetores no espaço vetorial a partir do valor do cosseno do ângulo compreendido entre eles. Se o ângulo compreendido for zero (ambos os vetores apontam para o mesmo lugar), a medida resultará no valor 1. Para um ângulo diferente de zero, o valor resultante será inferior a 1. Para vetores ortogonais8, o valor será zero. Se os vetores apontarem em direções contrárias, o valor será -1. Logo, a medida do Cosseno encontra-se no intervalo fechado entre [-1, 1]. Contudo, como os valores de frequência de termos (palavras) são positivos, o cosseno desses vetores encontra-se no intervalo entre [0, 1], sendo que quanto mais próximo de 1 for o valor, maior é a similaridade entre os vetores.\n",
            "Para fins de exemplificação, consideremos os dois textos do Exemplo 10.1:\n",
            "Exemplo 10.1  \n",
            "Em seguida, calculamos o comprimento de cada vetor com base na Equação 10.3:\n",
            "A medida do Cosseno é menos sensível à frequência de ocorrência das palavras em um corpus do que outras medidas de similaridade, como a Distância Euclidiana. Isso significa que as palavras menos frequentes não terão um peso desproporcional no cálculo da similaridade entre os vetores. Essa é principal razão que faz com que essa medida seja tão frequente na Semântica Distribucional e muito usada para calcular a similaridade entre vetores de palavras.\n",
            "Vimos nas Seções 10.1.1 e 10.1.2 que as matrizes termo-documento e termo-contexto associam a frequência de ocorrência de cada termo ao documento ou contexto em que ocorrem. No entanto, a frequência simples de um termo (isto é, o número de vezes que ele ocorre) é pouco discriminativa, já que algumas palavras (como “porque”, “durante”, “após”, “sobre” etc.) são bastante comuns e não caracterizam nenhum documento ou contexto em particular. Abordagens mais avançadas como é o caso das medidas TF-IDF (do inglês, Term Frequency-Inverse Document Frequency) e PMI (do inglês, Pointwise Mutual Information), costumam ser mais eficazes do que a simples frequência de um termo para discriminar o conteúdo de um documento ou um contexto. Como muitos termos nunca ocorrem em alguns documentos de uma coleção ou nunca aparecem em certos contextos, frequentemente, essas medidas levam a vetores com muitas dimensões e esparsos, ou seja, com muitos valores nulos (zeros). Por essa razão, as matrizes que se utilizam dessas medidas para atribuir valores aos termos são comumente chamadas de vetores esparsos. As medidas TF-IDF e PMI serão detalhadas nas Seções 10.2.1 e 10.2.2, respectivamente. Em seguida, na Seção 10.2.3, apresentamos o LSA (do inglês, Latent Semantic Analysis), um modelo muito adotado em PLN com o objetivo de reduzir a dimensionalidade de um espaço multidimensional criado com o uso do TF-IDF ou PMI.\n",
            "A medida TF-IDF representa uma alternativa mais eficiente do que a contagem de termos para atribuir valores aos termos de uma matriz termo-documento. Ela atribui um peso para cada termo de um documento multiplicando a frequência do termo no documento (TF) pelo inverso da frequência do termo em todos os documentos de um corpus ou coleção (IDF). Dessa maneira, um termo que ocorre muitas vezes em um documento, mas não em muitos documentos da coleção, terá um peso mais alto, enquanto um termo que ocorre em muitos documentos terá um peso mais baixo.\n",
            "Quanto menor o número de documentos que contêm determinado termo, maior será o TF-IDF daquele termo. Em suma, termos que aparecem com frequência em muitos documentos recebem um peso menor do que os termos mais específicos de um determinado documento. TF-IDF é uma medida bastante versátil e amplamente utilizada em várias tarefas que envolvem o processamento de textos. Alguns exemplos de aplicação mais comuns que podem ser citados são:\n",
            "Uma forma mais eficaz de pesar os termos de uma matriz termo-contexto, comparada à simples contagem de coocorrência de termos, é usar a medida PMI (do inglês, Pointwise Mutual Information). PMI é uma medida estatística que auxilia na identificação de palavras associadas. Dito de outra forma, ela mede qual é a probabilidade que dois termos ocorram juntos um do outro em relação à probabilidade de cada termo ocorrer de forma independente. Por exemplo, o termo “inteligência artificial” tem um significado específico quando as palavras “inteligência” e “artificial” aparecem juntas em um texto. Quando ocorrem isoladamente, essas duas palavras constroem outros significados.\n",
            "onde:\n",
            "A medida PMI leva em consideração, tanto a probabilidade conjunta dos termos (numerador da Equação 10.8), quanto a distribuição geral de cada termo no corpus em que estão sendo analisados (denominador da equação). Vale lembrar que a probabilidade de dois eventos independentes ocorrerem é dada pelo produto das probabilidades dos dois eventos9.\n",
            "Embora PPMI seja uma medida útil para quantificar a força da relação entre dois termos em um corpus, ela pode ser muito sensível à frequência dos termos, podendo superestimar a associação entre palavras raras e subestimar a associação entre palavras comuns. Outra limitação das medidas PMI e PPMI é que, por depender somente da coocorrência de palavras no corpus, não levam em consideração a posição das palavras no texto. Isso pode ser um problema em situações em que a ordem das palavras é importante como é o caso das expressões multipalavras.\n",
            "Medidas de associação de termos como PMI e PPMI são úteis em uma variedade de aplicações de processamento de linguagem natural, conforme exemplificamos a seguir:\n",
            "Os modelos TF-IDF e PMI/PPMI apresentados nas Seções 10.2.1 e 10.2.2 frequentemente geram matrizes esparsas contendo muitas células com valor nulo (zero), dado que muitos termos têm baixa frequência em muitos documentos (no caso da matriz termo-documento) ou não ocorrem em muitos contextos em um corpus (no caso da matriz termo-contexto). A esparsidade de uma matriz pode afetar significativamente o desempenho dos algoritmos que a processam, levando a um aumento no tempo de processamento, devido à necessidade de acessar todos valores armazenados, inclusive os valores nulos. Por conter muitos zeros, a matriz esparsa ainda demanda muita memória para poder armazenar todos os valores, mesmo com corpora relativamente pequenos.\n",
            "Para lidar com esse problema decorrente do grande número de dimensões, é comum o uso de técnicas que permitam reduzir a dimensionalidade de uma matriz. A Análise de Semântica Latente (em inglês, Latent Semantic Analysis – LSA) ou, ainda, Indexação de Semântica Latente (em inglês, Latent Semantic Indexing – LSI), como é chamada na área de recuperação de informação, pode ser aplicada para reduzir a dimensionalidade de um espaço multidimensional. O objetivo principal do LSA é reduzir a dimensão da matriz original, diminuindo a importância de valores singulares menores. Isso ajuda a eliminar ruídos e a capturar as relações semânticas subjacentes entre as palavras.\n",
            "\n",
            "Embora o LSA seja uma técnica poderosa para análise de texto e redução de dimensionalidade, o modelo também possui algumas limitações que devem ser consideradas. Por exemplo, ao tratar os termos como entidades independentes, ignorando as relações de contexto mais complexas que ocorrem em linguagem natural, nuances de significado que dependem do contexto podem não ser totalmente capturadas. Tratando as palavras de maneira independente, o modelo também não captura a estrutura sintática da linguagem.\n",
            "Outra limitação importante é que o LSA tende a ter dificuldade em lidar com documentos muito curtos, uma vez que a coocorrência de termos relevantes é menor e a representação no espaço de conceitos pode ser menos robusta. O LSA é uma abordagem estática, o que significa que não é capaz de lidar bem com mudanças no significado das palavras ao longo do tempo ou em diferentes contextos. Modelos mais recentes, como os baseados em redes neurais, podem lidar melhor com essas nuances.\n",
            "Embora o LSA capture informações semânticas latentes, os conceitos extraídos nem sempre são facilmente interpretáveis por seres humanos. Isso dificulta a compreensão do que exatamente está sendo capturado em cada dimensão reduzida do espaço de conceitos. Além disso, ele não é capaz de capturar nuances semânticas mais complexas, como a ambiguidade.\n",
            "Em resumo, o LSA é uma técnica valiosa para muitas tarefas de processamento de linguagem natural, mas é importante estar ciente de suas limitações e considerar outras abordagens, como modelos baseados em redes neurais, para lidar com algumas das desvantagens mencionadas.\n",
            "Vimos na Seção 10.2 como representar uma palavra por meio de um vetor esparso e com muitas dimensões, correspondentes às palavras do vocabulário ou aos documentos de uma coleção. Nesta seção, introduziremos uma representação de palavras mais robusta, conhecida por embeddings, de vetores densos e menores, com dimensões variando entre 50-1000. Essas dimensões não possuem uma interpretação clara do seu significado (Jurafsky; Martin, 2023).\n",
            "Os vetores são densos, ou seja, seus valores são números reais positivos ou negativos, ao invés de contagens esparsas, na maioria das vezes zeros, como é o caso dos vetores esparsos vistos na Seção 10.2. Vetores densos (daqui em diante, embeddings) capturam melhor as relações semânticas e contextuais entre as palavras do que os vetores esparsos. Por exemplo, na representação vetorial esparsa, sinônimos como “alfabeto” e “abecedário” muito provavelmente têm dimensões distintas e não relacionadas, pois esse tipo de modelo pode falhar ao capturar a similaridade entre palavras que estão no contexto de “alfabeto” e “abecedário”. Essa é uma das razões que faz com que os embeddings apresentem melhor desempenho em tarefas de PLN do que os vetores esparsos.\n",
            "Os embeddings são aprendidos a partir de corpora por meio de algoritmos de aprendizado de máquina supervisionado ou não supervisionado, por exemplo, usando redes neurais artificiais como é o caso do modelo Word2Vec (Seção 10.3.1), ou, ainda, usando representação estatística da matriz de coocorrência de termos, como é o caso do modelo Glove (Seção 10.3.3).\n",
            "Os vetores de embeddings podem ser estáticos ou dinâmicos. Os embeddings estáticos permanecem fixos uma vez aprendidos, ou seja, eles não podem ser ajustados ou modificados para uma tarefa específica. Ao contrário desses, os embeddings dinâmicos podem ser ajustados em tarefas específicas, se adaptando às nuances específicas da tarefa e ao contexto atual. A escolha entre essas abordagens depende das necessidades da aplicação, do domínio e das características das tarefas em que os embeddings serão utilizados.\n",
            "Nesta seção o foco será apenas na descrição dos embeddings estáticos, mais especificamente, nos modelos Word2Vec (Seção 10.3.1), Fasttext (Seção 10.3.2) e GloVe (Seção 10.3.3). Os embeddings dinâmicos como os modelos ElMo, BERT e GPT são abordados no Capítulo 15.\n",
            "Os embeddings estáticos podem ser definidos para diversos tipos de unidades de representação, incluindo palavras, caracteres, subpalavras, sentenças e até mesmo textos com várias sentenças. Por exemplo, considere as sentenças do Exemplo 10.2:\n",
            "Exemplo 10.2  \n",
            "Considerando o contexto da sentença e o senso comum, o termo “banco” na primeira sentença corresponde a uma instituição financeira cujo significado é distinto do “banco” da segunda sentença que corresponde a um assento. Neste caso, os embeddings estáticos definem um mesmo vetor para representar a palavra “banco” nas duas sentenças, independente do contexto.\n",
            "Embora a ideia de representar elementos de um texto usando vetores no espaço multidimensional não seja tão recente (vide, por exemplo, (Joos, 1950), (Harris, 1954), (Firth, 1957), (Osgood; Suci; Tenenbaum, 1957)), somente a partir de 2013 os embeddings começaram a ser muito utilizados, com o desenvolvimento e a disponibilização do modelo Word2Vec (Mikolov et al., 2013), conforme explicado a seguir.\n",
            "O Word2Vec é uma técnica de aprendizado de unidades de representações distribuídas proposta por Mikolov et al. (2013), que tem como objetivo capturar a semântica e a relação entre unidades de representação em um corpus, aprendendo embeddings estáticos para cada palavra presente no vocabulário de treino (Jurafsky; Martin, 2023). O modelo é baseado na ideia de que palavras que ocorrem em contextos semelhantes têm significados semelhantes. Portanto, ele explora a distribuição de palavras em grandes corpora para aprender representações vetoriais que capturam esses padrões.\n",
            "O Word2Vec possui duas arquiteturas principais: CBOW – Continuous Bag-of-Words e Skip-gram. Na arquitetura CBOW, o modelo tenta prever uma palavra-alvo com base em um contexto de várias palavras de entrada. O contexto é definido por um conjunto de palavras vizinhas da palavra-alvo. Ao contrário do CBOW, na arquitetura Skip-gram o modelo tenta prever o contexto (palavras vizinhas) de uma palavra-alvo. Dito de outra forma, o Skip-gram tenta encontrar as palavras que normalmente aparecem no contexto da palavra-alvo. Esse método é, em geral, mais lento de treinar, mas muitas vezes gera representações mais precisas.\n",
            "As subseções 10.3.1.1 e 10.3.1.2 explicam as arquiteturas CBOW e Skip-gram, respectivamente.\n",
            "A ideia principal por trás do CBOW é prever a palavra-alvo com base no contexto. O contexto é definido por um conjunto de palavras vizinhas da palavra-alvo. Por exemplo, na frase “Vou ao banco sacar dinheiro.”, o CBOW tentaria prever a palavra “banco” com base no contexto [Vou, ao, sacar, dinheiro], sendo [Vou, ao] o conjunto de palavras anteriores a palavra-alvo e [sacar, dinheiro] o conjunto de palavras posteriores. Dessa forma, o modelo aprende a associação entre as palavras do contexto e a palavra-alvo.\n",
            "\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "Fonte: Adaptado de (Mikolov et al., 2013)\n",
            "O softmax é uma generalização da função sigmoid que converte um vetor numérico em um vetor de probabilidades de possíveis saídas (Jurafsky; Martin, 2023), com valores dentro do intervalo [0,1] perfazendo um somatório de 1, como representado na Equação 10.11.\n",
            "Assim, a função softmax tem por objetivo converter um vetor numérico em um vetor normalizado de probabilidades.\n",
            "Enquanto o modelo CBOW prevê a representação de uma unidade (e.g. palavra) com base nos contextos em que ela ocorre em um corpus de treinamento, o modelo Skip-gram tenta prever o contexto (ou as representações vizinhas) a partir de um alvo. Considerando as representações como palavras. Nesta arquitetura, cada palavra é uma entrada para uma rede neural similar à arquitetura do CBOW, com camada de projeção para prever palavras dentro de um determinado intervalo antes e depois da palavra de entrada, conforme Figura 10.5. Utilizando o mesmo exemplo anterior, o Skip-gram recebe a palavra “banco” como entrada e tenta prever o contexto “Vou, ao, sacar, dinheiro”. Essa abordagem permite que o modelo aprenda a representação de uma palavra, considerando as palavras que normalmente aparecem ao seu redor.\n",
            "O Skip-gram parte da ideia de que é mais provável que uma palavra ocorra próxima a uma palavra-alvo se seu vetor for similar ao vetor desta palavra-alvo. Esse cálculo de similaridade é baseado na similaridade do Cosseno (Seção 10.1.3).\n",
            "Em suma, o Skip-gram treina um classificador que, dada uma palavra-alvo e seu contexto, calcula uma probabilidade do quão similares são o contexto e a palavra-alvo. Quanto mais distantes forem os vetores do contexto e da palavra-alvo, menos relacionados são a palavra-alvo e o contexto e, portanto, menor será o peso daquele contexto no treinamento (Mikolov et al., 2013).\n",
            "\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "Fonte: Adaptado de (Mikolov et al., 2013)\n",
            "O processo de treinamento do Word2Vec envolve a criação de um vocabulário a partir do corpus. Durante o treinamento, o modelo ajusta os valores desses vetores para maximizar a capacidade de prever a palavra-alvo ou o contexto, dependendo da arquitetura escolhida.\n",
            "Embora considerada mais complexa que a CBOW, a abordagem Skip-gram é mais comumente utilizada com a técnica de amostragem negativa (Skip-gram with Negative Sampling – SGNS). Na amostragem negativa apenas um subconjunto de palavras negativas (palavras não contextuais) é selecionado para a atualização dos pesos, ao invés de ajustar os pesos de todas as palavras no corpus a cada iteração. Isso torna o treinamento mais eficiente computacionalmente, aprendendo boas representações, especialmente para palavras mais frequentes (Mikolov et al., 2013).\n",
            "Uma vez treinado, o modelo Word2Vec é capaz de fornecer representações vetoriais para palavras (embeddings), nas quais palavras semanticamente similares são mapeadas para regiões próximas do espaço vetorial.\n",
            "Apesar das vantagens de se utilizar o modelo Word2Vec, é importante destacar que o mesmo possui algumas limitações. Embora o modelo Word2Vec capture as relações semânticas entre palavras, as palavras com múltiplos sentidos podem ser ambíguas dependendo do contexto, dificultando a precisão da representação.\n",
            "Outra limitação do Word2Vec é que ele não lida bem com palavras raras ou fora do vocabulário; consequentemente essas palavras podem gerar representações vetoriais pouco confiáveis. Além disso, a arquitetura Skip-gram não captura explicitamente relações sintáticas, tais como relações entre adjetivos, verbos, por exemplo. Sendo assim, para tarefas que exigem um entendimento mais profundo da estrutura gramatical, outros modelos ou técnicas podem ser mais adequados.\n",
            "Além da questão da representação de palavras do vocabulário, o modelo não leva em consideração questões morfológicas e ignora a estrutura interna das palavras, o que é uma limitação especialmente para línguas morfologicamente ricas, ou seja, que possuem uma grande variedade de morfemas que podem para expressar diferentes funções gramaticais e que podem ser adicionados, alterados ou combinados para criar diferentes formas de palavras e estruturas gramaticais dentro da língua, como Árabe ou Finlandês.\n",
            "Por fim, o Word2Vec não lida diretamente com a concatenação de palavras como uma única unidade. Por exemplo, o modelo não conseguiria reconhecer a palavra “pontapé”, se ela não estivesse presente no vocabulário, mesmo se o vocabulário de treino contivesse as palavras “ponta” e “pé”. No entanto, existem variações do modelo que permitem capturar informações contextuais mais ricas, como o modelo Fasttext descrito a seguir.\n",
            "O Fasttext é uma extensão do modelo Word2Vec criada pelo grupo Facebook AI Research (FAIR), que amplia o conceito do Skip-gram, no qual cada palavra é representada como uma combinação de n-gramas de caracteres. O modelo leva em consideração não apenas as palavras individuais, mas também as subpalavras (morfemas) que compõem as palavras. Isso permite que o Fasttext consiga melhor representar as palavras fora do vocabulário e consequentemente, captura informações contextuais mais ricas, especialmente em idiomas com aglutinação e morfologia complexa.\n",
            "A principal diferença em relação ao Word2Vec está no tratamento das palavras, ou seja, ao invés de tratá-las como uma única unidade, o Fasttext as divide em n-gramas menores, como n-gramas de caracteres, e as representa como a soma de seus n-gramas.\n",
            "Mais especificamente, cada palavra w é representada como um saco de caracteres (bag of characters). Para cada palavra, são adicionados os símbolos ‘<’ e ‘>’ denotando o início e fim das palavras, respectivamente. Isso permite a distinção entre prefixos, sufixos e outras sequências de caracteres. Além disso, a própria palavra w é incluída no conjunto de n-gramas, para aprender um embedding para cada palavra além dos n-gramas dos caracteres.\n",
            "\n",
            "Em termos arquiteturais, o FastText também possui duas arquiteturas assim como o Word2Vec: CBOW – Continuous Bag-of-Words e Skip-gram.\n",
            "\n",
            "A Figura 10.7 exemplifica o funcionamento do FastText com a arquitetura CBOW. Essa arquitetura prediz a palavra-alvo com base no contexto. Cada embedding é gerado a partir dos vetores dos n-gramas, neste exemplo, sendo bigramas.\n",
            "Embora o FastText seja um modelo eficiente e útil para muitas tarefas de processamento de linguagem natural, ele também possui algumas limitações. A primeira é a representação limitada do contexto. Nesse modelo, os textos são representados como a soma das representações de vetor de palavras individuais. Isso significa que o Fasttext não captura informações de ordem ou estrutura sintática mais complexa nos textos.\n",
            "Outra limitação importante é que o modelo é sensível ao tamanho do vocabulário. Em outras palavras, o FastText requer uma representação de vetor de palavras predefinida para cada palavra no vocabulário. Isso pode levar a problemas de dimensionamento, quando se lida com vocabulários muito grandes, pois o espaço vetorial se torna maior e o treinamento e a inferência podem se tornar mais lentos e exigentes em recursos computacionais.\n",
            "Embora o FastText seja eficaz em muitos idiomas, pode não funcionar tão bem em idiomas com morfologia muito complexa, onde as palavras se desdobram em várias formas com significados diferentes. Em tais casos, modelos que incorporam informações morfológicas mais profundas podem ser mais apropriados.\n",
            "É importante notar que as limitações do FastText não o tornam inadequado para todas as tarefas de processamento de linguagem natural. Ele continua sendo uma escolha sólida para muitos cenários devido à sua eficiência e simplicidade, mas é importante considerar essas limitações ao decidir qual modelo utilizar em um projeto específico. Em tarefas mais complexas e em idiomas com características particulares, pode ser necessário explorar modelos mais avançados como os modelos contextualizados abordados no Capítulo 15.\n",
            "Ao explorar modelos de embeddings estáticos como Word2Vec e FastText, é importante mencionar outra abordagem: o modelo GloVe (Global Vectors for Word Representation) (Pennington; Socher; Manning, 2014). Enquanto o Word2Vec e o FastText se concentram principalmente na relação local entre as palavras, o GloVe adota uma perspectiva global, levando em consideração a contagem de coocorrência palavra-palavra em um corpus. Essa abordagem permite que o GloVe capture informações de relação semântica e sintática entre as palavras.\n",
            "O GloVe é um modelo global de regressão log-bilinear, que relaciona as variáveis dependentes e independentes por meio de uma função logarítmica (Pennington; Socher; Manning, 2014). Uma função log-bilinear é uma função não linear que tem como argumentos dois vetores. Neste modelo, a regressão log-bilinear é aplicada para estimar os vetores de palavras e as matrizes de transformação necessárias para mapear as palavras em um espaço vetorial.\n",
            "Primeiramente, uma matriz de coocorrência é construída a partir de um corpus de textos. Essa matriz registra quantas vezes duas palavras aparecem juntas em uma janela de contexto. A partir da matriz de coocorrência, é construída uma matriz de probabilidade que representa a probabilidade condicional de uma palavra ocorrer perto de outra palavra. Essa matriz tenta capturar a relação entre as palavras considerando suas frequências relativas de coocorrência. O objetivo do GloVe é encontrar representações vetoriais para palavras de forma que a relação entre os vetores corresponda à relação entre suas probabilidades de coocorrência. Isso é formulado como uma função de perda que minimiza o erro entre as relações de coocorrência reais e as estimadas. O modelo é treinado ajustando os vetores de palavras para minimizar a função de perda. Isso é feito usando um algoritmo de otimização, o Gradiente Descendente Estocástico (Stochastic Gradient Descent – SGD). O SGD é um algoritmo de otimização para ajustar os parâmetros de um modelo de acordo com uma função de custo a ser minimizada.\n",
            "Ao utilizar uma combinação das vantagens dos métodos existentes (fatoração de matriz global e janela de contexto local), há uma análise das propriedades dos modelos que não eram totalmente exploradas e argumentam que o ponto de partida apropriado para o aprendizado de embeddings deve ser com proporções de probabilidades de coocorrência, ao invés das próprias probabilidades (Pennington; Socher; Manning, 2014).\n",
            "1ª propriedade:\n",
            "2ª propriedade:\n",
            "3ª propriedade:\n",
            "O resultado do modelo é um conjunto de representações vetoriais de palavras que capturam as relações entre as palavras em termos semânticos e sintáticos. Esses vetores podem ser usados em diversas tarefas de PLN.\n",
            "O GloVe tem capacidade para capturar informações semânticas mais abrangentes em comparação aos outros métodos de representação vetorial de palavras, devido à abordagem de coocorrência global ponderada.\n",
            "Apesar das vantagens do GloVe em relação às tarefas de analogia e similaridade, cabe destacar algumas limitações do método. Em virtude de seu treinamento necessitar de uma matriz de contagem de coocorrência palavra-palavra, o modelo consome muito espaço de memória; além disso, assim como o Word2Vec, o GloVe não consegue lidar com palavras fora do vocabulário e ignora a morfologia das palavras.\n",
            "Em síntese, GloVe é um modelo de representação vetorial de palavras que utiliza informações de coocorrência global entre palavras no corpus e estima os parâmetros do modelo por meio de regressão log-bilinear. Essa abordagem visa capturar relações mais abrangentes entre as palavras e obter vetores de palavras mais informativos e precisos.\n",
            "As representações vetoriais geradas pelos modelos Word2Vec, Fasttext e GloVe têm uma dimensionalidade menor em comparação com os vetores esparsos baseados em abordagens como TF-IDF e PMI/PMMI, o que ajuda a reduzir o custo computacional e a dimensionalidade dos dados. Outra vantagem dos embeddings é que eles podem ser transferidos e usados como recursos em tarefas de aprendizado de máquina relacionadas, melhorando o desempenho de modelos em tarefas de NLP com conjuntos de dados menores.\n",
            "Embora os embeddings apresentem algumas vantagens em relação aos modelos esparsos, nos embeddings estáticos cada palavra tem uma única representação, independentemente do contexto em que a palavra ocorre. Isso limita a capacidade de compreender a ambiguidade nas palavras. Além disso, por se tratarem de modelos estáticos, ou seja, que não são atualizados durante o treinamento da tarefa na qual são empregados, eles podem não se adaptar bem a tarefas específicas, especialmente aquelas que exigem informações contextuais específicas da tarefa como é o caso da análise de sentimentos, resolução de ambiguidades e tradução automática. Há que se considerar, ainda, que a língua natural está em constante evolução, e o significado das palavras pode mudar ao longo do tempo. Os embeddings estáticos não conseguem capturar essas mudanças.\n",
            "Para tentar contornar esses problemas dos embeddings estáticos, permitindo o ajuste do modelo durante o treinamento da tarefa específica, várias abordagens baseadas em embeddings dinâmicos têm sido estudadas na literatura recentemente. Ao contrário dos vetores estáticos, os vetores dinâmicos se adaptam ao vocabulário e às características específicas da tarefa, capturando informações contextuais, pois consideram o contexto em que cada palavra é usada em um documento ou sentença. Isso permite lidar melhor com palavras polissêmicas, tornando os embeddings mais adequados para tarefas que dependem do contexto.\n",
            "Os modelos de embeddings dinâmicos são o assunto do Capítulo 15.\n",
            "Sentenças retiradas de títulos de notícias retornadas pelo Google, em 18/04/2023, a partir dos termos de busca “ensino” e “educação” combinados com “aluno”, “escola” e “professor”.↩︎\n",
            "http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc↩︎\n",
            "O termo “documento” é usado nesta seção de forma genérica, podendo se referir a sentenças, parágrafos ou documentos completos.↩︎\n",
            "https://revistapesquisa.fapesp.br/↩︎\n",
            "https://revistapesquisa.fapesp.br/a-forca-das-renovaveis/↩︎\n",
            "https://revistapesquisa.fapesp.br/sob-o-risco-da-escassez/↩︎\n",
            "https://revistapesquisa.fapesp.br/eletricos-movidos-a-etanol/↩︎\n",
            "Dois vetores são ortogonais se o Produto Escalar entre eles é nulo (zero), conforme veremos na Equação 10.1.↩︎\n",
            "Os dois últimos termos da Equação 10.8 são considerados equivalentes pelo Teorema de Bayes.↩︎\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extração do conteudo do capitulo 16\n",
        "main_16 = soup_16.find('main', class_='content')\n",
        "paragrafos_16 = []\n",
        "for paragrafo in main_16.find_all(sem_span):\n",
        "  paragrafos_16.append(paragrafo.text)\n",
        "\n",
        "for i in paragrafos:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsqaoUzrghfE",
        "outputId": "c3f3e3ef-f7ca-471d-808a-8b6c07f09a13"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eloize Seno \n",
            "Daniela Claro \n",
            "Laila Mota \n",
            "Jessica Rodrigues \n",
            "26/09/2023\n",
            "É relativamente fácil para nós, seres humanos, visualizarmos um texto e, a partir de uma simples leitura, extrairmos dele determinados tipos de informação. Por exemplo, ao ler o texto “Ser feliz sem motivo é a forma mais autêntica de felicidade.” podemos reconhecer o sentido das palavras e o significado do texto formado pela junção de todas as palavras. Diferente dos humanos, os algoritmos computacionais não conseguem processar símbolos/palavras. Ao invés disso, eles requerem uma representação numérica de um documento ou texto a ser processado, para que consigam realizar suas operações.\n",
            "A semântica distribucional tem sido atualmente a principal abordagem de representação do significado lexical adotada nas mais diversas tarefas do processamento de linguagem natural. Nessa abordagem, os itens lexicais (palavras) são representados por meio de vetores de valores reais, conhecidos por vetores semânticos, que codificam o significado das palavras a partir de sua distribuição em textos.\n",
            "A semântica distribucional é ancorada na Hipótese Distribucional (Firth, 1957; Harris, 1954) que preconiza que palavras que têm um contexto linguístico semelhante tendem a ter significado similar ou aproximado. É o caso, por exemplo, de palavras como “ensino” e “educação” que costumam aparecer no mesmo contexto de palavras como “aluno”, “escola” e “professor”, sugerindo que existe uma similaridade entre as duas palavras em certos contextos. Vejamos, por exemplo, a ocorrência dessas palavras nas sentenças a seguir1:\n",
            "Assim, na semântica distribucional as palavras são caracterizadas pelo contexto em que elas aparecem. Por se basearem em distribuição, os vetores semânticos podem ser aprendidos automaticamente a partir de textos, sem que haja supervisão de um humano (utilizando-se textos não rotulados, portanto). Os modelos que aprendem esse tipo de representação são denominados de Modelos Semânticos Distribucionais - MSD (Distributional Semantic Models – DSM, no inglês).\n",
            "Os MSD são, frequentemente, classificados como vetores esparsos e vetores densos. Por exemplo, o modelo TF-IDF, amplamente adotado em tarefas que envolvem a comparação de similaridade entre documentos, como a detecção de plágio, a inferência textual e a recuperação de informações, é um exemplo clássico de vetor esparso. Nesse modelo, o significado de uma palavra é representado por meio de uma função simples calculada com base na frequência da palavra em uma coleção de documentos, conforme será visto Seção 10.2.1. Como muitas palavras nunca ocorrem em alguns documentos, frequentemente, esse modelo leva a vetores muito grandes e esparsos, ou seja, com muitos zeros. Por outro lado, os modelos da família Word2Vec (Mikolov et al., 2013) são considerados vetores densos (não esparsos), onde as entradas são números reais que representam propriedades semânticas úteis (conforme será abordado na Seção 10.3.1, ao invés de contagens quase zero.\n",
            "A Figura 10.1 ilustra a estrutura geral dos Modelos Semânticos Distribucionais. Neste capítulo serão abordados apenas os vetores esparsos e os vetores densos estáticos. Mais especificamente, iniciaremos apresentando os modelos esparsos TF-IDF e PMI (Seção 10.2) e depois apresentaremos os principais vetores densos estáticos como o Word2Vec, o GloVe e o Fasttext (Seção 10.3). Os vetores densos dinâmicos, por sua vez, como ELMO, BERT e GPT serão abordados no Capítulo 15. Antes, porém, de apresentar os MSD, introduziremos alguns conceitos fundamentais da semântica vetorial e apresentaremos a similaridade do cosseno (Seção 10.1), uma maneira padrão de usar os vetores semânticos para calcular a similaridade entre palavras, sentenças e documentos, que é uma ferramenta fundamental em aplicações práticas como a sumarização automática e a recuperação de informações.\n",
            "\n",
            "As primeiras investigações no campo da semântica vetorial, também conhecida por métodos distribucionais, tiveram início na década de 1950, impulsionadas pela convicção de que o significado de uma palavra pode ser definido a partir da sua distribuição nos contextos linguísticos em que ela ocorre, compartilhada por linguistas como (Joos, 1950), (Harris, 1954) e (Firth, 1957), e pela proposta de (Osgood; Suci; Tenenbaum, 1957) de usar um ponto no espaço multidimensional para representar a conotação de uma palavra. Portanto, os métodos distribucionais se definem como uma representação vetorial que retrata o significado de uma palavra a partir da distribuição das palavras que formam o seu contexto (Jurafsky; Martin, 2023). Por exemplo, a Figura 10.2 representa o espaço vetorial semântico da palavra “ensino”2. Palavras como “educação”, “estudantes”, “professores” e “alunos” são alguns exemplos de palavras que compartilham esse mesmo espaço semântico.\n",
            "\n",
            "A representação vetorial semântica, ou simplesmente vetores semânticos, é um padrão de representação muito usual em PLN, que pode retratar vários aspectos do significado das palavras, como a similaridade (ex. “comércio” e “negócio”); a orientação de sentimento ou polaridade (ex. “fenomenal”, que conota uma avaliação positiva, e “estúpido”, que conota uma avaliação negativa); a associação entre palavras (ex. “futebol” e “bola”, que são claramente relacionados, uma vez que futebol se joga com uma bola), entre outros aspectos.\n",
            "A ideia dos vetores semânticos é, então, representar cada palavra como um ponto em um espaço vetorial multidimensional, construído a partir da distribuição de suas palavras vizinhas.\n",
            "Espaços vetoriais são objetos de estudo da Álgebra Linear e são bem caracterizados pela sua dimensão, que, grosseiramente falando, representa o número de direções independentes no espaço. Um espaço vetorial é formado por uma coleção de objetos chamados vetores. Em um Modelo Semântico Distribucional é possível representar palavras, sentenças e até documentos completos como vetores em um espaço multidimensional.\n",
            "Geralmente, os vetores semânticos são representados por meio de uma matriz de coocorrência (ou distribuição de coocorrência), que retrata a frequência de coocorrência das palavras. As representações matriciais mais comuns são a matriz termo-documento, onde cada dimensão (vetor) da matriz representa um documento, e a matriz termo-contexto, onde cada dimensão representa uma palavra (Jurafsky; Martin, 2023). As subseções a seguir abordam essas duas formas básicas de representação.\n",
            "A matriz termo-documento foi definida por (Salton; Allan, 1994) como parte do Modelo de Espaço Vetorial – MEV proposto pelos autores para a recuperação de documentos na web. No MEV, um documento é representado como um vetor de frequência de palavras (ou termos), uma coluna, como no exemplo da Tabela 1. Os três documentos representados na tabela, por meio de colunas, representam pontos em um espaço tridimensional. A matriz foi computada a partir de três textos de divulgação científica extraídos da revista online Pesquisa FAPESP4, da seção de Tecnologia, edições 308 e 310 de 2021. Os textos se referem aos seguintes temas: energias renováveis5 (coluna 1), risco de escassez de energia fornecida pelas hidrelétricas brasileiras6 (coluna 2) e veículos elétricos movidos a etanol7 (coluna 3). A matriz da Tabela 10.1 representa apenas um subconjunto das palavras que ocorreram nesses textos. A contagem (frequência) das palavras em cada texto foi realizada considerando a sua forma lematizada.\n",
            "\n",
            "\n",
            "Em aplicações reais, os vocabulários têm milhares de palavras e o número de documentos pode ser enorme (imagine todas as páginas da web). Isso frequentemente resulta em vetores muito grandes, levando a matrizes esparsas, já que muitas palavras nunca aparecem em outros documentos. Para lidar com um grande número de dimensões, uma técnica comumente usada é a Análise Semântica Latente (em inglês, Latent Semantic Analysis – LSA), que reduz a dimensionalidade do espaço vetorial através da Decomposição em Valores Singulares (em inglês, Singular Value Decomposition – SVD), conforme veremos na Seção 10.2.3.\n",
            "Os vetores semânticos também podem ser usados para representar o significado de palavras e não apenas de documentos. Para tanto, ao invés de usarmos uma matriz de termo-documento (como visto na Seção 10.1.1), usamos uma matriz de termo-contexto, também conhecida por matriz palavra-palavra ou matriz termo-termo.\n",
            "A Tabela 10.2 apresenta um subconjunto simplificado da matriz de coocorrência termo-contexto computada a partir dos textos da revista Pesquisa FAPESP descritos na Seção 10.1.1. As linhas da tabela representam cada palavra-alvo e cada célula indica o número de vezes que a palavra-alvo correspondente coocorreu em cada contexto (colunas). Para a criação da matriz foi considerada uma janela de contexto de tamanho 5, isto é, cinco palavras à esquerda e cinco palavras à direita da palavra-alvo. Para exemplificação, o vetor da palavra-alvo “energia” está destacado em negrito.\n",
            "\n",
            "\n",
            "Os vetores que representam palavras são frequentemente denominados de embeddings, embora muitas vezes esse termo seja usado de maneira mais restrita para se referir apenas aos vetores densos como é o caso do Word2Vec, Glove e Fastext (Seção 10.3), e não aos vetores esparsos como o TF-IDF e o PMI (Seção 10.2).\n",
            "Uma tarefa bastante comum do processamento de linguagem natural consiste em calcular a similaridade entre vetores de documentos ou vetores de palavras, seja para estabelecer uma métrica de semelhança entre dois textos ou para se ter uma medida de equivalência entre duas palavras. Para tanto, faz-se necessário o emprego de alguma medida de similaridade entre vetores.\n",
            "A medida do Cosseno, também conhecida por distância do Cosseno, é sem dúvida uma das mais clássicas da área de PLN. Essa medida calcula a distância entre dois vetores no espaço vetorial a partir do valor do cosseno do ângulo compreendido entre eles. Se o ângulo compreendido for zero (ambos os vetores apontam para o mesmo lugar), a medida resultará no valor 1. Para um ângulo diferente de zero, o valor resultante será inferior a 1. Para vetores ortogonais8, o valor será zero. Se os vetores apontarem em direções contrárias, o valor será -1. Logo, a medida do Cosseno encontra-se no intervalo fechado entre [-1, 1]. Contudo, como os valores de frequência de termos (palavras) são positivos, o cosseno desses vetores encontra-se no intervalo entre [0, 1], sendo que quanto mais próximo de 1 for o valor, maior é a similaridade entre os vetores.\n",
            "Para fins de exemplificação, consideremos os dois textos do Exemplo 10.1:\n",
            "Exemplo 10.1  \n",
            "Em seguida, calculamos o comprimento de cada vetor com base na Equação 10.3:\n",
            "A medida do Cosseno é menos sensível à frequência de ocorrência das palavras em um corpus do que outras medidas de similaridade, como a Distância Euclidiana. Isso significa que as palavras menos frequentes não terão um peso desproporcional no cálculo da similaridade entre os vetores. Essa é principal razão que faz com que essa medida seja tão frequente na Semântica Distribucional e muito usada para calcular a similaridade entre vetores de palavras.\n",
            "Vimos nas Seções 10.1.1 e 10.1.2 que as matrizes termo-documento e termo-contexto associam a frequência de ocorrência de cada termo ao documento ou contexto em que ocorrem. No entanto, a frequência simples de um termo (isto é, o número de vezes que ele ocorre) é pouco discriminativa, já que algumas palavras (como “porque”, “durante”, “após”, “sobre” etc.) são bastante comuns e não caracterizam nenhum documento ou contexto em particular. Abordagens mais avançadas como é o caso das medidas TF-IDF (do inglês, Term Frequency-Inverse Document Frequency) e PMI (do inglês, Pointwise Mutual Information), costumam ser mais eficazes do que a simples frequência de um termo para discriminar o conteúdo de um documento ou um contexto. Como muitos termos nunca ocorrem em alguns documentos de uma coleção ou nunca aparecem em certos contextos, frequentemente, essas medidas levam a vetores com muitas dimensões e esparsos, ou seja, com muitos valores nulos (zeros). Por essa razão, as matrizes que se utilizam dessas medidas para atribuir valores aos termos são comumente chamadas de vetores esparsos. As medidas TF-IDF e PMI serão detalhadas nas Seções 10.2.1 e 10.2.2, respectivamente. Em seguida, na Seção 10.2.3, apresentamos o LSA (do inglês, Latent Semantic Analysis), um modelo muito adotado em PLN com o objetivo de reduzir a dimensionalidade de um espaço multidimensional criado com o uso do TF-IDF ou PMI.\n",
            "A medida TF-IDF representa uma alternativa mais eficiente do que a contagem de termos para atribuir valores aos termos de uma matriz termo-documento. Ela atribui um peso para cada termo de um documento multiplicando a frequência do termo no documento (TF) pelo inverso da frequência do termo em todos os documentos de um corpus ou coleção (IDF). Dessa maneira, um termo que ocorre muitas vezes em um documento, mas não em muitos documentos da coleção, terá um peso mais alto, enquanto um termo que ocorre em muitos documentos terá um peso mais baixo.\n",
            "Quanto menor o número de documentos que contêm determinado termo, maior será o TF-IDF daquele termo. Em suma, termos que aparecem com frequência em muitos documentos recebem um peso menor do que os termos mais específicos de um determinado documento. TF-IDF é uma medida bastante versátil e amplamente utilizada em várias tarefas que envolvem o processamento de textos. Alguns exemplos de aplicação mais comuns que podem ser citados são:\n",
            "Uma forma mais eficaz de pesar os termos de uma matriz termo-contexto, comparada à simples contagem de coocorrência de termos, é usar a medida PMI (do inglês, Pointwise Mutual Information). PMI é uma medida estatística que auxilia na identificação de palavras associadas. Dito de outra forma, ela mede qual é a probabilidade que dois termos ocorram juntos um do outro em relação à probabilidade de cada termo ocorrer de forma independente. Por exemplo, o termo “inteligência artificial” tem um significado específico quando as palavras “inteligência” e “artificial” aparecem juntas em um texto. Quando ocorrem isoladamente, essas duas palavras constroem outros significados.\n",
            "onde:\n",
            "A medida PMI leva em consideração, tanto a probabilidade conjunta dos termos (numerador da Equação 10.8), quanto a distribuição geral de cada termo no corpus em que estão sendo analisados (denominador da equação). Vale lembrar que a probabilidade de dois eventos independentes ocorrerem é dada pelo produto das probabilidades dos dois eventos9.\n",
            "Embora PPMI seja uma medida útil para quantificar a força da relação entre dois termos em um corpus, ela pode ser muito sensível à frequência dos termos, podendo superestimar a associação entre palavras raras e subestimar a associação entre palavras comuns. Outra limitação das medidas PMI e PPMI é que, por depender somente da coocorrência de palavras no corpus, não levam em consideração a posição das palavras no texto. Isso pode ser um problema em situações em que a ordem das palavras é importante como é o caso das expressões multipalavras.\n",
            "Medidas de associação de termos como PMI e PPMI são úteis em uma variedade de aplicações de processamento de linguagem natural, conforme exemplificamos a seguir:\n",
            "Os modelos TF-IDF e PMI/PPMI apresentados nas Seções 10.2.1 e 10.2.2 frequentemente geram matrizes esparsas contendo muitas células com valor nulo (zero), dado que muitos termos têm baixa frequência em muitos documentos (no caso da matriz termo-documento) ou não ocorrem em muitos contextos em um corpus (no caso da matriz termo-contexto). A esparsidade de uma matriz pode afetar significativamente o desempenho dos algoritmos que a processam, levando a um aumento no tempo de processamento, devido à necessidade de acessar todos valores armazenados, inclusive os valores nulos. Por conter muitos zeros, a matriz esparsa ainda demanda muita memória para poder armazenar todos os valores, mesmo com corpora relativamente pequenos.\n",
            "Para lidar com esse problema decorrente do grande número de dimensões, é comum o uso de técnicas que permitam reduzir a dimensionalidade de uma matriz. A Análise de Semântica Latente (em inglês, Latent Semantic Analysis – LSA) ou, ainda, Indexação de Semântica Latente (em inglês, Latent Semantic Indexing – LSI), como é chamada na área de recuperação de informação, pode ser aplicada para reduzir a dimensionalidade de um espaço multidimensional. O objetivo principal do LSA é reduzir a dimensão da matriz original, diminuindo a importância de valores singulares menores. Isso ajuda a eliminar ruídos e a capturar as relações semânticas subjacentes entre as palavras.\n",
            "\n",
            "Embora o LSA seja uma técnica poderosa para análise de texto e redução de dimensionalidade, o modelo também possui algumas limitações que devem ser consideradas. Por exemplo, ao tratar os termos como entidades independentes, ignorando as relações de contexto mais complexas que ocorrem em linguagem natural, nuances de significado que dependem do contexto podem não ser totalmente capturadas. Tratando as palavras de maneira independente, o modelo também não captura a estrutura sintática da linguagem.\n",
            "Outra limitação importante é que o LSA tende a ter dificuldade em lidar com documentos muito curtos, uma vez que a coocorrência de termos relevantes é menor e a representação no espaço de conceitos pode ser menos robusta. O LSA é uma abordagem estática, o que significa que não é capaz de lidar bem com mudanças no significado das palavras ao longo do tempo ou em diferentes contextos. Modelos mais recentes, como os baseados em redes neurais, podem lidar melhor com essas nuances.\n",
            "Embora o LSA capture informações semânticas latentes, os conceitos extraídos nem sempre são facilmente interpretáveis por seres humanos. Isso dificulta a compreensão do que exatamente está sendo capturado em cada dimensão reduzida do espaço de conceitos. Além disso, ele não é capaz de capturar nuances semânticas mais complexas, como a ambiguidade.\n",
            "Em resumo, o LSA é uma técnica valiosa para muitas tarefas de processamento de linguagem natural, mas é importante estar ciente de suas limitações e considerar outras abordagens, como modelos baseados em redes neurais, para lidar com algumas das desvantagens mencionadas.\n",
            "Vimos na Seção 10.2 como representar uma palavra por meio de um vetor esparso e com muitas dimensões, correspondentes às palavras do vocabulário ou aos documentos de uma coleção. Nesta seção, introduziremos uma representação de palavras mais robusta, conhecida por embeddings, de vetores densos e menores, com dimensões variando entre 50-1000. Essas dimensões não possuem uma interpretação clara do seu significado (Jurafsky; Martin, 2023).\n",
            "Os vetores são densos, ou seja, seus valores são números reais positivos ou negativos, ao invés de contagens esparsas, na maioria das vezes zeros, como é o caso dos vetores esparsos vistos na Seção 10.2. Vetores densos (daqui em diante, embeddings) capturam melhor as relações semânticas e contextuais entre as palavras do que os vetores esparsos. Por exemplo, na representação vetorial esparsa, sinônimos como “alfabeto” e “abecedário” muito provavelmente têm dimensões distintas e não relacionadas, pois esse tipo de modelo pode falhar ao capturar a similaridade entre palavras que estão no contexto de “alfabeto” e “abecedário”. Essa é uma das razões que faz com que os embeddings apresentem melhor desempenho em tarefas de PLN do que os vetores esparsos.\n",
            "Os embeddings são aprendidos a partir de corpora por meio de algoritmos de aprendizado de máquina supervisionado ou não supervisionado, por exemplo, usando redes neurais artificiais como é o caso do modelo Word2Vec (Seção 10.3.1), ou, ainda, usando representação estatística da matriz de coocorrência de termos, como é o caso do modelo Glove (Seção 10.3.3).\n",
            "Os vetores de embeddings podem ser estáticos ou dinâmicos. Os embeddings estáticos permanecem fixos uma vez aprendidos, ou seja, eles não podem ser ajustados ou modificados para uma tarefa específica. Ao contrário desses, os embeddings dinâmicos podem ser ajustados em tarefas específicas, se adaptando às nuances específicas da tarefa e ao contexto atual. A escolha entre essas abordagens depende das necessidades da aplicação, do domínio e das características das tarefas em que os embeddings serão utilizados.\n",
            "Nesta seção o foco será apenas na descrição dos embeddings estáticos, mais especificamente, nos modelos Word2Vec (Seção 10.3.1), Fasttext (Seção 10.3.2) e GloVe (Seção 10.3.3). Os embeddings dinâmicos como os modelos ElMo, BERT e GPT são abordados no Capítulo 15.\n",
            "Os embeddings estáticos podem ser definidos para diversos tipos de unidades de representação, incluindo palavras, caracteres, subpalavras, sentenças e até mesmo textos com várias sentenças. Por exemplo, considere as sentenças do Exemplo 10.2:\n",
            "Exemplo 10.2  \n",
            "Considerando o contexto da sentença e o senso comum, o termo “banco” na primeira sentença corresponde a uma instituição financeira cujo significado é distinto do “banco” da segunda sentença que corresponde a um assento. Neste caso, os embeddings estáticos definem um mesmo vetor para representar a palavra “banco” nas duas sentenças, independente do contexto.\n",
            "Embora a ideia de representar elementos de um texto usando vetores no espaço multidimensional não seja tão recente (vide, por exemplo, (Joos, 1950), (Harris, 1954), (Firth, 1957), (Osgood; Suci; Tenenbaum, 1957)), somente a partir de 2013 os embeddings começaram a ser muito utilizados, com o desenvolvimento e a disponibilização do modelo Word2Vec (Mikolov et al., 2013), conforme explicado a seguir.\n",
            "O Word2Vec é uma técnica de aprendizado de unidades de representações distribuídas proposta por Mikolov et al. (2013), que tem como objetivo capturar a semântica e a relação entre unidades de representação em um corpus, aprendendo embeddings estáticos para cada palavra presente no vocabulário de treino (Jurafsky; Martin, 2023). O modelo é baseado na ideia de que palavras que ocorrem em contextos semelhantes têm significados semelhantes. Portanto, ele explora a distribuição de palavras em grandes corpora para aprender representações vetoriais que capturam esses padrões.\n",
            "O Word2Vec possui duas arquiteturas principais: CBOW – Continuous Bag-of-Words e Skip-gram. Na arquitetura CBOW, o modelo tenta prever uma palavra-alvo com base em um contexto de várias palavras de entrada. O contexto é definido por um conjunto de palavras vizinhas da palavra-alvo. Ao contrário do CBOW, na arquitetura Skip-gram o modelo tenta prever o contexto (palavras vizinhas) de uma palavra-alvo. Dito de outra forma, o Skip-gram tenta encontrar as palavras que normalmente aparecem no contexto da palavra-alvo. Esse método é, em geral, mais lento de treinar, mas muitas vezes gera representações mais precisas.\n",
            "As subseções 10.3.1.1 e 10.3.1.2 explicam as arquiteturas CBOW e Skip-gram, respectivamente.\n",
            "A ideia principal por trás do CBOW é prever a palavra-alvo com base no contexto. O contexto é definido por um conjunto de palavras vizinhas da palavra-alvo. Por exemplo, na frase “Vou ao banco sacar dinheiro.”, o CBOW tentaria prever a palavra “banco” com base no contexto [Vou, ao, sacar, dinheiro], sendo [Vou, ao] o conjunto de palavras anteriores a palavra-alvo e [sacar, dinheiro] o conjunto de palavras posteriores. Dessa forma, o modelo aprende a associação entre as palavras do contexto e a palavra-alvo.\n",
            "\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "Fonte: Adaptado de (Mikolov et al., 2013)\n",
            "O softmax é uma generalização da função sigmoid que converte um vetor numérico em um vetor de probabilidades de possíveis saídas (Jurafsky; Martin, 2023), com valores dentro do intervalo [0,1] perfazendo um somatório de 1, como representado na Equação 10.11.\n",
            "Assim, a função softmax tem por objetivo converter um vetor numérico em um vetor normalizado de probabilidades.\n",
            "Enquanto o modelo CBOW prevê a representação de uma unidade (e.g. palavra) com base nos contextos em que ela ocorre em um corpus de treinamento, o modelo Skip-gram tenta prever o contexto (ou as representações vizinhas) a partir de um alvo. Considerando as representações como palavras. Nesta arquitetura, cada palavra é uma entrada para uma rede neural similar à arquitetura do CBOW, com camada de projeção para prever palavras dentro de um determinado intervalo antes e depois da palavra de entrada, conforme Figura 10.5. Utilizando o mesmo exemplo anterior, o Skip-gram recebe a palavra “banco” como entrada e tenta prever o contexto “Vou, ao, sacar, dinheiro”. Essa abordagem permite que o modelo aprenda a representação de uma palavra, considerando as palavras que normalmente aparecem ao seu redor.\n",
            "O Skip-gram parte da ideia de que é mais provável que uma palavra ocorra próxima a uma palavra-alvo se seu vetor for similar ao vetor desta palavra-alvo. Esse cálculo de similaridade é baseado na similaridade do Cosseno (Seção 10.1.3).\n",
            "Em suma, o Skip-gram treina um classificador que, dada uma palavra-alvo e seu contexto, calcula uma probabilidade do quão similares são o contexto e a palavra-alvo. Quanto mais distantes forem os vetores do contexto e da palavra-alvo, menos relacionados são a palavra-alvo e o contexto e, portanto, menor será o peso daquele contexto no treinamento (Mikolov et al., 2013).\n",
            "\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "Fonte: Adaptado de (Mikolov et al., 2013)\n",
            "O processo de treinamento do Word2Vec envolve a criação de um vocabulário a partir do corpus. Durante o treinamento, o modelo ajusta os valores desses vetores para maximizar a capacidade de prever a palavra-alvo ou o contexto, dependendo da arquitetura escolhida.\n",
            "Embora considerada mais complexa que a CBOW, a abordagem Skip-gram é mais comumente utilizada com a técnica de amostragem negativa (Skip-gram with Negative Sampling – SGNS). Na amostragem negativa apenas um subconjunto de palavras negativas (palavras não contextuais) é selecionado para a atualização dos pesos, ao invés de ajustar os pesos de todas as palavras no corpus a cada iteração. Isso torna o treinamento mais eficiente computacionalmente, aprendendo boas representações, especialmente para palavras mais frequentes (Mikolov et al., 2013).\n",
            "Uma vez treinado, o modelo Word2Vec é capaz de fornecer representações vetoriais para palavras (embeddings), nas quais palavras semanticamente similares são mapeadas para regiões próximas do espaço vetorial.\n",
            "Apesar das vantagens de se utilizar o modelo Word2Vec, é importante destacar que o mesmo possui algumas limitações. Embora o modelo Word2Vec capture as relações semânticas entre palavras, as palavras com múltiplos sentidos podem ser ambíguas dependendo do contexto, dificultando a precisão da representação.\n",
            "Outra limitação do Word2Vec é que ele não lida bem com palavras raras ou fora do vocabulário; consequentemente essas palavras podem gerar representações vetoriais pouco confiáveis. Além disso, a arquitetura Skip-gram não captura explicitamente relações sintáticas, tais como relações entre adjetivos, verbos, por exemplo. Sendo assim, para tarefas que exigem um entendimento mais profundo da estrutura gramatical, outros modelos ou técnicas podem ser mais adequados.\n",
            "Além da questão da representação de palavras do vocabulário, o modelo não leva em consideração questões morfológicas e ignora a estrutura interna das palavras, o que é uma limitação especialmente para línguas morfologicamente ricas, ou seja, que possuem uma grande variedade de morfemas que podem para expressar diferentes funções gramaticais e que podem ser adicionados, alterados ou combinados para criar diferentes formas de palavras e estruturas gramaticais dentro da língua, como Árabe ou Finlandês.\n",
            "Por fim, o Word2Vec não lida diretamente com a concatenação de palavras como uma única unidade. Por exemplo, o modelo não conseguiria reconhecer a palavra “pontapé”, se ela não estivesse presente no vocabulário, mesmo se o vocabulário de treino contivesse as palavras “ponta” e “pé”. No entanto, existem variações do modelo que permitem capturar informações contextuais mais ricas, como o modelo Fasttext descrito a seguir.\n",
            "O Fasttext é uma extensão do modelo Word2Vec criada pelo grupo Facebook AI Research (FAIR), que amplia o conceito do Skip-gram, no qual cada palavra é representada como uma combinação de n-gramas de caracteres. O modelo leva em consideração não apenas as palavras individuais, mas também as subpalavras (morfemas) que compõem as palavras. Isso permite que o Fasttext consiga melhor representar as palavras fora do vocabulário e consequentemente, captura informações contextuais mais ricas, especialmente em idiomas com aglutinação e morfologia complexa.\n",
            "A principal diferença em relação ao Word2Vec está no tratamento das palavras, ou seja, ao invés de tratá-las como uma única unidade, o Fasttext as divide em n-gramas menores, como n-gramas de caracteres, e as representa como a soma de seus n-gramas.\n",
            "Mais especificamente, cada palavra w é representada como um saco de caracteres (bag of characters). Para cada palavra, são adicionados os símbolos ‘<’ e ‘>’ denotando o início e fim das palavras, respectivamente. Isso permite a distinção entre prefixos, sufixos e outras sequências de caracteres. Além disso, a própria palavra w é incluída no conjunto de n-gramas, para aprender um embedding para cada palavra além dos n-gramas dos caracteres.\n",
            "\n",
            "Em termos arquiteturais, o FastText também possui duas arquiteturas assim como o Word2Vec: CBOW – Continuous Bag-of-Words e Skip-gram.\n",
            "\n",
            "A Figura 10.7 exemplifica o funcionamento do FastText com a arquitetura CBOW. Essa arquitetura prediz a palavra-alvo com base no contexto. Cada embedding é gerado a partir dos vetores dos n-gramas, neste exemplo, sendo bigramas.\n",
            "Embora o FastText seja um modelo eficiente e útil para muitas tarefas de processamento de linguagem natural, ele também possui algumas limitações. A primeira é a representação limitada do contexto. Nesse modelo, os textos são representados como a soma das representações de vetor de palavras individuais. Isso significa que o Fasttext não captura informações de ordem ou estrutura sintática mais complexa nos textos.\n",
            "Outra limitação importante é que o modelo é sensível ao tamanho do vocabulário. Em outras palavras, o FastText requer uma representação de vetor de palavras predefinida para cada palavra no vocabulário. Isso pode levar a problemas de dimensionamento, quando se lida com vocabulários muito grandes, pois o espaço vetorial se torna maior e o treinamento e a inferência podem se tornar mais lentos e exigentes em recursos computacionais.\n",
            "Embora o FastText seja eficaz em muitos idiomas, pode não funcionar tão bem em idiomas com morfologia muito complexa, onde as palavras se desdobram em várias formas com significados diferentes. Em tais casos, modelos que incorporam informações morfológicas mais profundas podem ser mais apropriados.\n",
            "É importante notar que as limitações do FastText não o tornam inadequado para todas as tarefas de processamento de linguagem natural. Ele continua sendo uma escolha sólida para muitos cenários devido à sua eficiência e simplicidade, mas é importante considerar essas limitações ao decidir qual modelo utilizar em um projeto específico. Em tarefas mais complexas e em idiomas com características particulares, pode ser necessário explorar modelos mais avançados como os modelos contextualizados abordados no Capítulo 15.\n",
            "Ao explorar modelos de embeddings estáticos como Word2Vec e FastText, é importante mencionar outra abordagem: o modelo GloVe (Global Vectors for Word Representation) (Pennington; Socher; Manning, 2014). Enquanto o Word2Vec e o FastText se concentram principalmente na relação local entre as palavras, o GloVe adota uma perspectiva global, levando em consideração a contagem de coocorrência palavra-palavra em um corpus. Essa abordagem permite que o GloVe capture informações de relação semântica e sintática entre as palavras.\n",
            "O GloVe é um modelo global de regressão log-bilinear, que relaciona as variáveis dependentes e independentes por meio de uma função logarítmica (Pennington; Socher; Manning, 2014). Uma função log-bilinear é uma função não linear que tem como argumentos dois vetores. Neste modelo, a regressão log-bilinear é aplicada para estimar os vetores de palavras e as matrizes de transformação necessárias para mapear as palavras em um espaço vetorial.\n",
            "Primeiramente, uma matriz de coocorrência é construída a partir de um corpus de textos. Essa matriz registra quantas vezes duas palavras aparecem juntas em uma janela de contexto. A partir da matriz de coocorrência, é construída uma matriz de probabilidade que representa a probabilidade condicional de uma palavra ocorrer perto de outra palavra. Essa matriz tenta capturar a relação entre as palavras considerando suas frequências relativas de coocorrência. O objetivo do GloVe é encontrar representações vetoriais para palavras de forma que a relação entre os vetores corresponda à relação entre suas probabilidades de coocorrência. Isso é formulado como uma função de perda que minimiza o erro entre as relações de coocorrência reais e as estimadas. O modelo é treinado ajustando os vetores de palavras para minimizar a função de perda. Isso é feito usando um algoritmo de otimização, o Gradiente Descendente Estocástico (Stochastic Gradient Descent – SGD). O SGD é um algoritmo de otimização para ajustar os parâmetros de um modelo de acordo com uma função de custo a ser minimizada.\n",
            "Ao utilizar uma combinação das vantagens dos métodos existentes (fatoração de matriz global e janela de contexto local), há uma análise das propriedades dos modelos que não eram totalmente exploradas e argumentam que o ponto de partida apropriado para o aprendizado de embeddings deve ser com proporções de probabilidades de coocorrência, ao invés das próprias probabilidades (Pennington; Socher; Manning, 2014).\n",
            "1ª propriedade:\n",
            "2ª propriedade:\n",
            "3ª propriedade:\n",
            "O resultado do modelo é um conjunto de representações vetoriais de palavras que capturam as relações entre as palavras em termos semânticos e sintáticos. Esses vetores podem ser usados em diversas tarefas de PLN.\n",
            "O GloVe tem capacidade para capturar informações semânticas mais abrangentes em comparação aos outros métodos de representação vetorial de palavras, devido à abordagem de coocorrência global ponderada.\n",
            "Apesar das vantagens do GloVe em relação às tarefas de analogia e similaridade, cabe destacar algumas limitações do método. Em virtude de seu treinamento necessitar de uma matriz de contagem de coocorrência palavra-palavra, o modelo consome muito espaço de memória; além disso, assim como o Word2Vec, o GloVe não consegue lidar com palavras fora do vocabulário e ignora a morfologia das palavras.\n",
            "Em síntese, GloVe é um modelo de representação vetorial de palavras que utiliza informações de coocorrência global entre palavras no corpus e estima os parâmetros do modelo por meio de regressão log-bilinear. Essa abordagem visa capturar relações mais abrangentes entre as palavras e obter vetores de palavras mais informativos e precisos.\n",
            "As representações vetoriais geradas pelos modelos Word2Vec, Fasttext e GloVe têm uma dimensionalidade menor em comparação com os vetores esparsos baseados em abordagens como TF-IDF e PMI/PMMI, o que ajuda a reduzir o custo computacional e a dimensionalidade dos dados. Outra vantagem dos embeddings é que eles podem ser transferidos e usados como recursos em tarefas de aprendizado de máquina relacionadas, melhorando o desempenho de modelos em tarefas de NLP com conjuntos de dados menores.\n",
            "Embora os embeddings apresentem algumas vantagens em relação aos modelos esparsos, nos embeddings estáticos cada palavra tem uma única representação, independentemente do contexto em que a palavra ocorre. Isso limita a capacidade de compreender a ambiguidade nas palavras. Além disso, por se tratarem de modelos estáticos, ou seja, que não são atualizados durante o treinamento da tarefa na qual são empregados, eles podem não se adaptar bem a tarefas específicas, especialmente aquelas que exigem informações contextuais específicas da tarefa como é o caso da análise de sentimentos, resolução de ambiguidades e tradução automática. Há que se considerar, ainda, que a língua natural está em constante evolução, e o significado das palavras pode mudar ao longo do tempo. Os embeddings estáticos não conseguem capturar essas mudanças.\n",
            "Para tentar contornar esses problemas dos embeddings estáticos, permitindo o ajuste do modelo durante o treinamento da tarefa específica, várias abordagens baseadas em embeddings dinâmicos têm sido estudadas na literatura recentemente. Ao contrário dos vetores estáticos, os vetores dinâmicos se adaptam ao vocabulário e às características específicas da tarefa, capturando informações contextuais, pois consideram o contexto em que cada palavra é usada em um documento ou sentença. Isso permite lidar melhor com palavras polissêmicas, tornando os embeddings mais adequados para tarefas que dependem do contexto.\n",
            "Os modelos de embeddings dinâmicos são o assunto do Capítulo 15.\n",
            "Sentenças retiradas de títulos de notícias retornadas pelo Google, em 18/04/2023, a partir dos termos de busca “ensino” e “educação” combinados com “aluno”, “escola” e “professor”.↩︎\n",
            "http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc↩︎\n",
            "O termo “documento” é usado nesta seção de forma genérica, podendo se referir a sentenças, parágrafos ou documentos completos.↩︎\n",
            "https://revistapesquisa.fapesp.br/↩︎\n",
            "https://revistapesquisa.fapesp.br/a-forca-das-renovaveis/↩︎\n",
            "https://revistapesquisa.fapesp.br/sob-o-risco-da-escassez/↩︎\n",
            "https://revistapesquisa.fapesp.br/eletricos-movidos-a-etanol/↩︎\n",
            "Dois vetores são ortogonais se o Produto Escalar entre eles é nulo (zero), conforme veremos na Equação 10.1.↩︎\n",
            "Os dois últimos termos da Equação 10.8 são considerados equivalentes pelo Teorema de Bayes.↩︎\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Padrões utilizados para a identificação de erros no texto, em respectiva ordem:\n",
        "#Espaços repetidos, tabulações inadequadas, erros de alinhamento,\n",
        "#abreviações não definidas, erros de citação, erros de acentuação em letras maiúsculas,\n",
        "#erro de acentuação em letras maiúsculas no meio da palavra, concordância de gênero,\n",
        "#concordância de grau, concordância de pessoa, pontuação duplicada,\n",
        "#erro de espaçamento após pontuação e uso incorreto de aspas.\n",
        "\n",
        "padroes = [r'\\s{2,}', r'(?<=\\s)\\t+|\\t+(?=\\s)', r'^[ \\t]+|[ \\t]+$', r'\\b[A-Z]{2,}\\b',\n",
        "           r'(\\w+) \\((\\d{4})\\), \"([^\"]+)\" \\(p\\. (\\d+)\\)',\n",
        "           r'[ÁÉÍÓÚÀÃÊÔ][ÁÉÍÓÚÀÃÊÔ]*[a-z]', r'\\b\\w+[ÁÉÍÓÚÀÃÊÔ]\\w*\\b',\n",
        "           r'\\b[O|A] [a-zA-Z]+\\s+[O|A] [a-zA-Z]+\\b',\n",
        "           r'\\b[O|Os] [a-zA-Z]+\\s+[a-zA-Z]+\\s+[é|são]\\b',\n",
        "           r'\\b([A-Z][a-z]+)\\s+[a-zA-Z]+\\s+([a-z]+)\\b',\n",
        "           r'\\.{2,}', r'[.,!?](?=[^\\s])', r\"'[^']+'|\\\"[^\\\"]+\\\"\"]\n",
        "erros = []\n",
        "\n",
        "\n",
        "#Identificação dos erros através dos padrões listados acimas.\n",
        "for paragrafo in paragrafos:\n",
        "  for padrao in padroes:\n",
        "    erro = re.findall(padrao, paragrafo)\n",
        "    if len(erro) != 0:\n",
        "      erros.append(erro)\n",
        "\n",
        "for paragrafo in paragrafos_16:\n",
        "  for padrao in padroes:\n",
        "    erro = re.findall(padrao, paragrafo)\n",
        "    if len(erro) != 0:\n",
        "      erros.append(erro)\n",
        "\n",
        "print('Erros: ' + str(erros) + '\\n' + 'Quantidade de erros: ' + str(len(erros)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CL2j8ahO4yZ",
        "outputId": "829af824-25c0-4c0d-c244-50a758312091"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erros: [[' '], [' '], [' '], [' '], [('Ser', 'sem'), ('Diferente', 'humanos')], ['.'], ['MSD', 'DSM'], [('Por', 'basearem'), ('Os', 'que')], ['MSD', 'TF', 'IDF'], [('Como', 'palavras'), ('Por', 'lado'), ('Mikolov', 'al')], ['.', '.', '.', '.', '.'], ['TF', 'IDF', 'PMI', 'ELMO', 'BERT', 'GPT', 'MSD'], [('Os', 'densos')], ['.', '.', '.', '.'], ['.'], ['PLN'], ['Ál'], ['MEV', 'MEV'], [('Os', 'se')], ['.'], ['LSA', 'SVD'], [('Isso', 'resulta'), ('Para', 'com')], ['.', '.'], ['.', '.'], ['FAPESP'], [('Pesquisa', 'descritos'), ('As', 'da')], ['.', '.', '.'], ['TF', 'IDF', 'PMI'], [('Os', 'que')], ['.', '.'], [('Uma', 'bastante')], ['PLN'], [('Essa', 'calcula'), ('Se', 'vetores')], [('Para', 'de')], ['.'], [' \\xa0'], ['.'], ['.'], [('Isso', 'que'), ('Distribucional', 'muito')], ['TF', 'IDF', 'PMI', 'TF', 'IDF', 'PMI', 'LSA', 'PLN', 'TF', 'IDF', 'PMI'], [('Como', 'termos')], ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'], ['TF', 'IDF', 'TF', 'IDF'], [('Ela', 'um')], ['TF', 'IDF', 'TF', 'IDF'], [('Quanto', 'o'), ('Alguns', 'de')], ['PMI', 'PMI'], [('Uma', 'mais'), ('Dito', 'outra'), ('Quando', 'isoladamente')], ['PMI'], [('Vale', 'que')], ['.'], ['PPMI', 'PMI', 'PPMI'], [('Embora', 'seja'), ('Isso', 'ser')], ['PMI', 'PPMI'], ['TF', 'IDF', 'PMI', 'PPMI'], [('Por', 'muitos')], ['.', '.', '.', '.'], ['LSA', 'LSI', 'LSA'], [('Para', 'com'), ('Isso', 'a')], ['LSA'], [('Tratando', 'palavras')], ['LSA', 'LSA'], [('Modelos', 'recentes')], ['LSA'], [('Isso', 'a')], ['LSA'], ['.'], ['PLN'], ['.'], ['.', '.', '.', '.'], [('Os', 'de')], ['BERT', 'GPT'], ['.', '.', '.', '.', '.', '.'], ['.'], [' \\xa0'], ['.'], [('Considerando', 'contexto')], [('Embora', 'ideia'), ('Mikolov', 'al')], ['.'], [('Mikolov', 'al')], ['CBOW', 'CBOW', 'CBOW'], [('Dito', 'outra')], ['CBOW'], ['.', '.', '.', '.', '.', '.'], ['CBOW', 'CBOW'], [('Vou', 'banco')], ['.'], [('Mikolov', 'al')], ['.'], [',', '.'], ['CBOW', 'CBOW'], [('Enquanto', 'modelo'), ('Utilizando', 'mesmo'), ('Essa', 'permite')], ['.', '.'], ['.', '.'], [('Quanto', 'distantes'), ('Mikolov', 'al')], ['.'], [('Mikolov', 'al')], ['.'], [('Durante', 'treinamento')], ['CBOW', 'SGNS'], [('Embora', 'mais'), ('Na', 'negativa'), ('Isso', 'o'), ('Mikolov', 'al')], ['.'], [('Uma', 'treinado')], [('Apesar', 'vantagens'), ('Embora', 'modelo')], ['Ár'], [('Fasttext', 'a')], ['AI', 'FAIR'], [('Isso', 'que'), ('Fasttext', 'melhor')], [('Fasttext', 'divide')], [('Para', 'palavra'), ('Isso', 'a')], ['CBOW'], [('Em', 'arquiteturais')], ['CBOW'], [('Essa', 'prediz')], ['.'], [('Isso', 'que')], [('Em', 'palavras'), ('Isso', 'levar')], [('Em', 'casos')], [('Ele', 'sendo'), ('Em', 'mais')], [('Ao', 'modelos'), ('Global', 'for'), ('Essa', 'permite')], ['SGD', 'SGD'], [('Essa', 'registra'), ('Essa', 'tenta')], [('Ao', 'uma')], ['PLN'], [('Esses', 'podem')], [('Apesar', 'vantagens'), ('Em', 'de')], [('Essa', 'visa')], ['TF', 'IDF', 'PMI', 'PMMI', 'NLP'], [('Outra', 'dos')], [('Embora', 'embeddings'), ('Isso', 'a')], [('Para', 'contornar'), ('Isso', 'lidar')], [('Os', 'de')], ['.'], ['.', '.', '.', '.'], ['.'], ['.', '.'], ['.', '.'], ['.', '.'], ['.', '.'], [('Produto', 'entre')], ['.', '.'], ['.', '.'], [' '], [' '], [' '], [' '], [('Ser', 'sem'), ('Diferente', 'humanos')], ['.'], ['MSD', 'DSM'], [('Por', 'basearem'), ('Os', 'que')], ['MSD', 'TF', 'IDF'], [('Como', 'palavras'), ('Por', 'lado'), ('Mikolov', 'al')], ['.', '.', '.', '.', '.'], ['TF', 'IDF', 'PMI', 'ELMO', 'BERT', 'GPT', 'MSD'], [('Os', 'densos')], ['.', '.', '.', '.'], ['.'], ['PLN'], ['Ál'], ['MEV', 'MEV'], [('Os', 'se')], ['.'], ['LSA', 'SVD'], [('Isso', 'resulta'), ('Para', 'com')], ['.', '.'], ['.', '.'], ['FAPESP'], [('Pesquisa', 'descritos'), ('As', 'da')], ['.', '.', '.'], ['TF', 'IDF', 'PMI'], [('Os', 'que')], ['.', '.'], [('Uma', 'bastante')], ['PLN'], [('Essa', 'calcula'), ('Se', 'vetores')], [('Para', 'de')], ['.'], [' \\xa0'], ['.'], ['.'], [('Isso', 'que'), ('Distribucional', 'muito')], ['TF', 'IDF', 'PMI', 'TF', 'IDF', 'PMI', 'LSA', 'PLN', 'TF', 'IDF', 'PMI'], [('Como', 'termos')], ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'], ['TF', 'IDF', 'TF', 'IDF'], [('Ela', 'um')], ['TF', 'IDF', 'TF', 'IDF'], [('Quanto', 'o'), ('Alguns', 'de')], ['PMI', 'PMI'], [('Uma', 'mais'), ('Dito', 'outra'), ('Quando', 'isoladamente')], ['PMI'], [('Vale', 'que')], ['.'], ['PPMI', 'PMI', 'PPMI'], [('Embora', 'seja'), ('Isso', 'ser')], ['PMI', 'PPMI'], ['TF', 'IDF', 'PMI', 'PPMI'], [('Por', 'muitos')], ['.', '.', '.', '.'], ['LSA', 'LSI', 'LSA'], [('Para', 'com'), ('Isso', 'a')], ['LSA'], [('Tratando', 'palavras')], ['LSA', 'LSA'], [('Modelos', 'recentes')], ['LSA'], [('Isso', 'a')], ['LSA'], ['.'], ['PLN'], ['.'], ['.', '.', '.', '.'], [('Os', 'de')], ['BERT', 'GPT'], ['.', '.', '.', '.', '.', '.'], ['.'], [' \\xa0'], ['.'], [('Considerando', 'contexto')], [('Embora', 'ideia'), ('Mikolov', 'al')], ['.'], [('Mikolov', 'al')], ['CBOW', 'CBOW', 'CBOW'], [('Dito', 'outra')], ['CBOW'], ['.', '.', '.', '.', '.', '.'], ['CBOW', 'CBOW'], [('Vou', 'banco')], ['.'], [('Mikolov', 'al')], ['.'], [',', '.'], ['CBOW', 'CBOW'], [('Enquanto', 'modelo'), ('Utilizando', 'mesmo'), ('Essa', 'permite')], ['.', '.'], ['.', '.'], [('Quanto', 'distantes'), ('Mikolov', 'al')], ['.'], [('Mikolov', 'al')], ['.'], [('Durante', 'treinamento')], ['CBOW', 'SGNS'], [('Embora', 'mais'), ('Na', 'negativa'), ('Isso', 'o'), ('Mikolov', 'al')], ['.'], [('Uma', 'treinado')], [('Apesar', 'vantagens'), ('Embora', 'modelo')], ['Ár'], [('Fasttext', 'a')], ['AI', 'FAIR'], [('Isso', 'que'), ('Fasttext', 'melhor')], [('Fasttext', 'divide')], [('Para', 'palavra'), ('Isso', 'a')], ['CBOW'], [('Em', 'arquiteturais')], ['CBOW'], [('Essa', 'prediz')], ['.'], [('Isso', 'que')], [('Em', 'palavras'), ('Isso', 'levar')], [('Em', 'casos')], [('Ele', 'sendo'), ('Em', 'mais')], [('Ao', 'modelos'), ('Global', 'for'), ('Essa', 'permite')], ['SGD', 'SGD'], [('Essa', 'registra'), ('Essa', 'tenta')], [('Ao', 'uma')], ['PLN'], [('Esses', 'podem')], [('Apesar', 'vantagens'), ('Em', 'de')], [('Essa', 'visa')], ['TF', 'IDF', 'PMI', 'PMMI', 'NLP'], [('Outra', 'dos')], [('Embora', 'embeddings'), ('Isso', 'a')], [('Para', 'contornar'), ('Isso', 'lidar')], [('Os', 'de')], ['.'], ['.', '.', '.', '.'], ['.'], ['.', '.'], ['.', '.'], ['.', '.'], ['.', '.'], [('Produto', 'entre')], ['.', '.'], ['.', '.']]\n",
            "Quantidade de erros: 284\n"
          ]
        }
      ]
    }
  ]
}